# DeepFool presentation

#### Presenters: Chengkai Su, Faryab Haye, Zuoyi Li

#### Feb 18, 2021


## Paper
ATTA Paper:   [HERE](https://arxiv.org/pdf/1912.11969.pdf)

PGD Paper: [HERE](https://arxiv.org/pdf/1706.06083.pdf)

## Code reading guidance

Each of us focused on trying to replicate some result from the paper. These attempts are illustrated in the different jupyter notebooks in this repository. Out attempts include:

- **Running Time Demo (Chenkai):** This demo is intending to show Table 3 in the paper. The training is performed on the MNIST dataset with ATTA-MAT and PGD-MAT two methods. 
    - The code in the Jupyter Notebook is cloned from the [original ATTA repo](https://github.com/hzzheng93/ATTA.git). We make some changes to it to have the PGD-MAT method, which we forked and modified in this [repository](https://github.com/KyleSuchenkai/PGD.git)

- **PGD-k vs Accumulative PGD Demo (Faryab):** 
    - This demo attempts to replicate Figure 3 from the paper. 
    - Due to time/compute constraints we were only able to replicate the PGD-K line going from 0 to 20 attack iterations in each epoch, and the Accumulative-PGD-k line (for m = 10) going from 0 to 20 iterations attack iterations in each epoch.
    - The original paper did not signify how many epochs were ran in total, the codebase defaulted to training for 60 epochs for the whole training method, we instead trained for 50 epochs. This means that for the Accumulative-PGD-k line (for m = 10), accumulation started in the 40th epoch rather than the 50th one. 
    - The paper does not describe in detail what the loss value on the y-axis is calculated as. We assumed it means the sum of the cross-entropy-loss over the entire dataset in the last epoch.

- **Transferrability Demo (Zuoyi):** Based on the sections in the notebook:
    - *Section 1: Set up models:* Define classifier model for next step, and import MNIST data. 
    - *Section 2: Adversarially training a classifier using PGD-20:* Adversarially train a classifier using PGD-20 for 4 epochs, and attack this target model using PGD-20 to observe how much loss the attack introduces.
    - *Section 3: Adversarially training a classifier using ATTA-5:* Using ATTA-5 to train a fresh classifier for 4 epochs, and observe how much loss the accumulated ATTA-5 adversarial examples introduce to PGD-20 trained model.
    - *Section 4: Compare attacks from different adversarial training:* Compare the attacks generated by PGD-20 on the target model with ATTA-5’s. 

- **ATTA vs PGD-20 Demo (Zuoyi):** Based on the sections in the notebook:
    - *Section 1: Set up models:* Define classifier model for next step, and import MNIST data. 
    - *Section 2: Using PGD-20 to adversarially train a classifier:* Train a classifier for 4 epochs, saving data for intermediate models T1, T2, T3, T4.
    - *Section 3: calculate the loss and error transferability:* Using the data saved in the previous step to calculate the loss and error transferability as described in the paper. 


## Replication summary [consistency v.s. inconsistency]

- **Running Time Demo:** The result was similar to the paper’s result for both time and adversary training accuracy. 

- **PGD-k vs Accumulative PGD Demo:** There were some inconsistencies in this recreation.
    - Instead of going higher when the attack iterations in each epoch was increased, the like for m=10 actually started having lesser loss after 5 iterations per epoch. 
    - The loss values do not seem to go as high as the ones in the paper. At 20 attack iterations in each epoch, the m=1 line has a loss of about 0.05, where as in the original paper the loss is about 0.09. Similarly for the m=10 line the loss is about 0.15, where as in the original paper it is around 0.31. 
    - This may be attributed to us running the experiment for 10 epochs lesser, or using the wrong loss value from the one used in the paper.

- **Transferrability Demo:** 
    - From our reproduction, we observed strong transferability from T1 and T2, to T4. However, the transferability from T3 to T4 was relatively poor, although still much more than minuscule (0.2). 
    - The irregularity on T3 could be explained by the training epoch from T3 to T4 “unlearned” something, so the transferability seemed low. 
    - This is inconsistent to the original paper.

- **ATTA vs PGD-20 Demo:** 
    - PGD-20 attack introduced 5.56 cross entropy loss on the target model, while ATTA-5 attack after accumulating for 4 epochs introduced 4.96 loss. 
    - From this result we can state that ATTA accumulates strong attacks well in accumulating attack strength, and it also shows that ATTA can train a robust model with much less time compared to traditional methods. 
    - This is consistent with the original paper. 

