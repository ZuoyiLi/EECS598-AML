{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"presentation.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"15jTtYIr9mRTgiANcT3bybXsJDHApug9Z","authorship_tag":"ABX9TyNWg+rI/+nRtny/FYs0pVml"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Z5BLMDDc_3s0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618088607619,"user_tz":240,"elapsed":665,"user":{"displayName":"Kevin Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64","userId":"17607607416881687332"}},"outputId":"2046eb62-7a87-4b79-ad53-d3852bd823fe"},"source":["%cd '/content/drive/MyDrive/EECS 598 Adv. ML/Presentation1/nn_breaking_detection'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/EECS 598 Adv. ML/Presentation1/nn_breaking_detection\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H6pQA9Hsd62r"},"source":["# MNIST Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVVlIETSd36e","executionInfo":{"status":"ok","timestamp":1618088610427,"user_tz":240,"elapsed":1753,"user":{"displayName":"Kevin Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64","userId":"17607607416881687332"}},"outputId":"d333a2d4-7bee-4adb-df96-97d33a1b3e02"},"source":["## setup_mnist.py -- mnist data and model loading code\n","##\n","## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() \n","import numpy as np\n","import os\n","import pickle\n","import gzip\n","import urllib.request\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.utils import np_utils\n","from keras.models import load_model\n","\n","def extract_data(filename, num_images):\n","    with gzip.open(filename) as bytestream:\n","        bytestream.read(16)\n","        buf = bytestream.read(num_images*28*28)\n","        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n","        data = (data / 255) - 0.5\n","        data = data.reshape(num_images, 28, 28, 1)\n","        return data\n","\n","def extract_labels(filename, num_images):\n","    with gzip.open(filename) as bytestream:\n","        bytestream.read(8)\n","        buf = bytestream.read(1 * num_images)\n","        labels = np.frombuffer(buf, dtype=np.uint8)\n","    return (np.arange(10) == labels[:, None]).astype(np.float32)\n","\n","class MNIST:\n","    def __init__(self):\n","        if not os.path.exists(\"data\"):\n","            os.mkdir(\"data\")\n","            files = [\"train-images-idx3-ubyte.gz\",\n","                     \"t10k-images-idx3-ubyte.gz\",\n","                     \"train-labels-idx1-ubyte.gz\",\n","                     \"t10k-labels-idx1-ubyte.gz\"]\n","            for name in files:\n","\n","                urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/' + name, \"data/\"+name)\n","\n","        train_data = extract_data(\"data/train-images-idx3-ubyte.gz\", 60000)\n","        train_labels = extract_labels(\"data/train-labels-idx1-ubyte.gz\", 60000)\n","        self.test_data = extract_data(\"data/t10k-images-idx3-ubyte.gz\", 10000)\n","        self.test_labels = extract_labels(\"data/t10k-labels-idx1-ubyte.gz\", 10000)\n","        \n","        VALIDATION_SIZE = 5000\n","        \n","        self.validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n","        self.validation_labels = train_labels[:VALIDATION_SIZE]\n","        self.train_data = train_data[VALIDATION_SIZE:, :, :, :]\n","        self.train_labels = train_labels[VALIDATION_SIZE:]\n","\n","\n","class MNISTModel:\n","    def __init__(self, restore, session=None):\n","        self.num_channels = 1\n","        self.image_size = 28\n","        self.num_labels = 10\n","\n","        model = Sequential()\n","\n","        model.add(Conv2D(32, (3, 3),\n","                         input_shape=(28, 28, 1)))\n","        model.add(Activation('relu'))\n","        model.add(Conv2D(32, (3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        \n","        model.add(Conv2D(64, (3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(Conv2D(64, (3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        \n","        model.add(Flatten())\n","        model.add(Dense(200))\n","        model.add(Activation('relu'))\n","        model.add(Dense(200))\n","        model.add(Activation('relu'))\n","        model.add(Dense(10))\n","        model.load_weights(restore)\n","\n","        self.model = model\n","\n","    def predict(self, data):\n","        return self.model(data)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kU4M1NBMd4Ys"},"source":["# Train"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WngQJb5Ad0yw","executionInfo":{"status":"ok","timestamp":1618088812811,"user_tz":240,"elapsed":200998,"user":{"displayName":"Kevin Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64","userId":"17607607416881687332"}},"outputId":"11061ae6-2938-4754-b338-f6619a563bcd"},"source":["## train_models.py -- train the neural network models for attacking\n","##\n","## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.optimizers import SGD\n","\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() \n","from setup_mnist import MNIST\n","from setup_cifar import CIFAR\n","import os\n","\n","def train(data, file_name, params, num_epochs=50, batch_size=128, train_temp=1, init=None):\n","    \"\"\"\n","    Standard neural network training procedure.\n","    \"\"\"\n","    model = Sequential()\n","\n","    print(data.train_data.shape)\n","    \n","    model.add(Conv2D(params[0], (3, 3),\n","                            input_shape=data.train_data.shape[1:]))\n","    model.add(Activation('relu'))\n","    model.add(Conv2D(params[1], (3, 3)))\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","    model.add(Conv2D(params[2], (3, 3)))\n","    model.add(Activation('relu'))\n","    model.add(Conv2D(params[3], (3, 3)))\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","    model.add(Flatten())\n","    model.add(Dense(params[4]))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(params[5]))\n","    model.add(Activation('relu'))\n","    model.add(Dense(10))\n","    \n","    if init != None:\n","        model.load_weights(init)\n","\n","    def fn(correct, predicted):\n","        return tf.nn.softmax_cross_entropy_with_logits(labels=correct,\n","                                                       logits=predicted/train_temp)\n","\n","    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","    \n","    model.compile(loss=fn,\n","                  optimizer=sgd,\n","                  metrics=['accuracy'])\n","    \n","    model.fit(data.train_data, data.train_labels,\n","              batch_size=batch_size,\n","              validation_data=(data.validation_data, data.validation_labels),\n","              nb_epoch=num_epochs,\n","              shuffle=True)\n","    \n","\n","    if file_name != None:\n","        model.save(file_name)\n","\n","    return model\n","\n","def train_distillation(data, file_name, params, num_epochs=50, batch_size=128, train_temp=1):\n","    \"\"\"\n","    Train a network using defensive distillation.\n","\n","    Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks\n","    Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami\n","    IEEE S&P, 2016.\n","    \"\"\"\n","    if not os.path.exists(file_name+\"_init\"):\n","        # Train for one epoch to get a good starting point.\n","        train(data, file_name+\"_init\", params, 1, batch_size)\n","    \n","    # now train the teacher at the given temperature\n","    teacher = train(data, file_name+\"_teacher\", params, num_epochs, batch_size, train_temp,\n","                    init=file_name+\"_init\")\n","\n","    # evaluate the labels at temperature t\n","    predicted = teacher.predict(data.train_data)\n","    with tf.Session() as sess:\n","        y = sess.run(tf.nn.softmax(predicted/train_temp))\n","        print(y)\n","        data.train_labels = y\n","\n","    # train the student model at temperature t\n","    student = train(data, file_name, params, num_epochs, batch_size, train_temp,\n","                    init=file_name+\"_init\")\n","\n","    # and finally we predict at temperature 1\n","    predicted = student.predict(data.train_data)\n","\n","    print(predicted)\n","    \n","if not os.path.isdir('models'):\n","    os.makedirs('models')\n","\n","# train(CIFAR(), \"models/cifar\", [64, 64, 128, 128, 256, 256], num_epochs=50)\n","train(MNIST(), \"models/mnist\", [32, 32, 64, 64, 200, 200], num_epochs=15)\n","\n","train_distillation(MNIST(), \"models/mnist-distilled-100\", [32, 32, 64, 64, 200, 200],\n","                   num_epochs=15, train_temp=100)\n","# train_distillation(CIFAR(), \"models/cifar-distilled-100\", [64, 64, 128, 128, 256, 256],\n","#                   num_epochs=50, train_temp=100)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(55000, 28, 28, 1)\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","Train on 55000 samples, validate on 5000 samples\n","Epoch 1/15\n","55000/55000 [==============================] - ETA: 0s - loss: 0.7162 - acc: 0.7623"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"],"name":"stderr"},{"output_type":"stream","text":["55000/55000 [==============================] - 36s 650us/sample - loss: 0.7162 - acc: 0.7623 - val_loss: 0.0963 - val_acc: 0.9706\n","Epoch 2/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.1320 - acc: 0.9601 - val_loss: 0.0623 - val_acc: 0.9814\n","Epoch 3/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0921 - acc: 0.9726 - val_loss: 0.0532 - val_acc: 0.9848\n","Epoch 4/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0722 - acc: 0.9786 - val_loss: 0.0438 - val_acc: 0.9894\n","Epoch 5/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0613 - acc: 0.9811 - val_loss: 0.0447 - val_acc: 0.9866\n","Epoch 6/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0537 - acc: 0.9835 - val_loss: 0.0413 - val_acc: 0.9892\n","Epoch 7/15\n","55000/55000 [==============================] - 3s 60us/sample - loss: 0.0467 - acc: 0.9847 - val_loss: 0.0364 - val_acc: 0.9912\n","Epoch 8/15\n","55000/55000 [==============================] - 3s 60us/sample - loss: 0.0418 - acc: 0.9872 - val_loss: 0.0389 - val_acc: 0.9894\n","Epoch 9/15\n","55000/55000 [==============================] - 3s 60us/sample - loss: 0.0362 - acc: 0.9885 - val_loss: 0.0365 - val_acc: 0.9902\n","Epoch 10/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0322 - acc: 0.9895 - val_loss: 0.0331 - val_acc: 0.9918\n","Epoch 11/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0302 - acc: 0.9903 - val_loss: 0.0323 - val_acc: 0.9920\n","Epoch 12/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0285 - acc: 0.9912 - val_loss: 0.0325 - val_acc: 0.9922\n","Epoch 13/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0268 - acc: 0.9914 - val_loss: 0.0302 - val_acc: 0.9928\n","Epoch 14/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0257 - acc: 0.9916 - val_loss: 0.0300 - val_acc: 0.9922\n","Epoch 15/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0219 - acc: 0.9931 - val_loss: 0.0310 - val_acc: 0.9932\n","(55000, 28, 28, 1)\n","WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","Train on 55000 samples, validate on 5000 samples\n","Epoch 1/15\n","55000/55000 [==============================] - 4s 65us/sample - loss: 0.5423 - acc: 0.8948 - val_loss: 0.1092 - val_acc: 0.9674\n","Epoch 2/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.1491 - acc: 0.9545 - val_loss: 0.0784 - val_acc: 0.9772\n","Epoch 3/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.1170 - acc: 0.9636 - val_loss: 0.0719 - val_acc: 0.9798\n","Epoch 4/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.1008 - acc: 0.9689 - val_loss: 0.0629 - val_acc: 0.9828\n","Epoch 5/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0875 - acc: 0.9739 - val_loss: 0.0608 - val_acc: 0.9836\n","Epoch 6/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0826 - acc: 0.9754 - val_loss: 0.0520 - val_acc: 0.9854\n","Epoch 7/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0741 - acc: 0.9767 - val_loss: 0.0502 - val_acc: 0.9860\n","Epoch 8/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0703 - acc: 0.9779 - val_loss: 0.0488 - val_acc: 0.9872\n","Epoch 9/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0657 - acc: 0.9798 - val_loss: 0.0472 - val_acc: 0.9866\n","Epoch 10/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0598 - acc: 0.9810 - val_loss: 0.0469 - val_acc: 0.9878\n","Epoch 11/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0569 - acc: 0.9827 - val_loss: 0.0438 - val_acc: 0.9872\n","Epoch 12/15\n","55000/55000 [==============================] - 3s 60us/sample - loss: 0.0528 - acc: 0.9836 - val_loss: 0.0455 - val_acc: 0.9870\n","Epoch 13/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0515 - acc: 0.9841 - val_loss: 0.0443 - val_acc: 0.9894\n","Epoch 14/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0489 - acc: 0.9849 - val_loss: 0.0413 - val_acc: 0.9898\n","Epoch 15/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0464 - acc: 0.9852 - val_loss: 0.0410 - val_acc: 0.9900\n","[[8.2388336e-09 6.4478849e-07 8.6991098e-03 ... 6.3511235e-01\n","  4.7672984e-05 1.5000927e-03]\n"," [1.8538871e-17 4.5338750e-14 6.5477387e-13 ... 1.6695084e-13\n","  1.0833754e-06 9.3982081e-08]\n"," [8.3307666e-11 2.6003377e-07 3.7553594e-10 ... 6.9194602e-06\n","  2.1826020e-04 6.7923826e-01]\n"," ...\n"," [4.9761763e-14 2.2151074e-15 4.6237932e-18 ... 2.8070990e-15\n","  7.7614715e-09 5.6338404e-06]\n"," [6.5517934e-06 3.5523842e-10 7.7230283e-08 ... 5.2682949e-12\n","  6.1653495e-08 4.7518905e-11]\n"," [7.4367790e-08 1.7555526e-10 1.2598076e-07 ... 4.3155776e-08\n","  9.9999690e-01 2.8727666e-06]]\n","(55000, 28, 28, 1)\n","WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","Train on 55000 samples, validate on 5000 samples\n","Epoch 1/15\n","55000/55000 [==============================] - 4s 65us/sample - loss: 0.5343 - acc: 0.8992 - val_loss: 0.1090 - val_acc: 0.9688\n","Epoch 2/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.1353 - acc: 0.9582 - val_loss: 0.0862 - val_acc: 0.9754\n","Epoch 3/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.1070 - acc: 0.9672 - val_loss: 0.0708 - val_acc: 0.9790\n","Epoch 4/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0892 - acc: 0.9734 - val_loss: 0.0654 - val_acc: 0.9814\n","Epoch 5/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0776 - acc: 0.9775 - val_loss: 0.0621 - val_acc: 0.9824\n","Epoch 6/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0705 - acc: 0.9804 - val_loss: 0.0581 - val_acc: 0.9852\n","Epoch 7/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0667 - acc: 0.9816 - val_loss: 0.0585 - val_acc: 0.9854\n","Epoch 8/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0615 - acc: 0.9835 - val_loss: 0.0537 - val_acc: 0.9872\n","Epoch 9/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0586 - acc: 0.9842 - val_loss: 0.0545 - val_acc: 0.9870\n","Epoch 10/15\n","55000/55000 [==============================] - 3s 61us/sample - loss: 0.0560 - acc: 0.9855 - val_loss: 0.0543 - val_acc: 0.9864\n","Epoch 11/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0540 - acc: 0.9867 - val_loss: 0.0516 - val_acc: 0.9864\n","Epoch 12/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0511 - acc: 0.9877 - val_loss: 0.0492 - val_acc: 0.9896\n","Epoch 13/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0504 - acc: 0.9875 - val_loss: 0.0506 - val_acc: 0.9880\n","Epoch 14/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0481 - acc: 0.9891 - val_loss: 0.0514 - val_acc: 0.9884\n","Epoch 15/15\n","55000/55000 [==============================] - 3s 62us/sample - loss: 0.0480 - acc: 0.9892 - val_loss: 0.0502 - val_acc: 0.9884\n","[[ -637.5148    -268.0308     777.0034   ...  1130.8293     139.17467\n","    456.7443  ]\n"," [-1047.6174    -615.79706   -152.80138  ...  -373.2439    1183.7379\n","    963.636   ]\n"," [ -881.1698    -254.95009   -937.7262   ...   130.89795    534.5895\n","   1429.6113  ]\n"," ...\n"," [ -644.0845   -1114.6433   -1561.2822   ...  -947.7531     449.5445\n","   1150.4905  ]\n"," [  549.96106   -402.20917    151.42015  ...  -861.3802      -8.538276\n","   -761.52136 ]\n"," [  188.47717   -679.7217      87.91072  ...    88.0154    1676.9015\n","    410.05222 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cBd1KS2g3Fi6"},"source":["# CarliniL2"]},{"cell_type":"code","metadata":{"id":"Jha8cR2b3KwU","executionInfo":{"status":"ok","timestamp":1618088813209,"user_tz":240,"elapsed":196079,"user":{"displayName":"Kevin Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64","userId":"17607607416881687332"}}},"source":["import sys\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() \n","import numpy as np\n","\n","BINARY_SEARCH_STEPS = 9  # number of times to adjust the constant with binary search\n","MAX_ITERATIONS = 10000   # number of iterations to perform gradient descent\n","ABORT_EARLY = True       # if we stop improving, abort gradient descent early\n","LEARNING_RATE = 1e-2     # larger values converge faster to less accurate results\n","TARGETED = True          # should we target one specific class? or just be wrong?\n","CONFIDENCE = 0           # how strong the adversarial example should be\n","INITIAL_CONST = 1e-3     # the initial constant c to pick as a first guess\n","\n","class CarliniL2:\n","    def __init__(self, sess, model, batch_size=1, confidence = CONFIDENCE,\n","                 targeted = TARGETED, learning_rate = LEARNING_RATE,\n","                 binary_search_steps = BINARY_SEARCH_STEPS, max_iterations = MAX_ITERATIONS,\n","                 abort_early = ABORT_EARLY, \n","                 initial_const = INITIAL_CONST,\n","                 boxmin = -0.5, boxmax = 0.5):\n","        \"\"\"\n","        The L_2 optimized attack. \n","\n","        This attack is the most efficient and should be used as the primary \n","        attack to evaluate potential defenses.\n","\n","        Returns adversarial examples for the supplied model.\n","\n","        confidence: Confidence of adversarial examples: higher produces examples\n","          that are farther away, but more strongly classified as adversarial.\n","        batch_size: Number of attacks to run simultaneously.\n","        targeted: True if we should perform a targetted attack, False otherwise.\n","        learning_rate: The learning rate for the attack algorithm. Smaller values\n","          produce better results but are slower to converge.\n","        binary_search_steps: The number of times we perform binary search to\n","          find the optimal tradeoff-constant between distance and confidence. \n","        max_iterations: The maximum number of iterations. Larger values are more\n","          accurate; setting too small will require a large learning rate and will\n","          produce poor results.\n","        abort_early: If true, allows early aborts if gradient descent gets stuck.\n","        initial_const: The initial tradeoff-constant to use to tune the relative\n","          importance of distance and confidence. If binary_search_steps is large,\n","          the initial constant is not important.\n","        boxmin: Minimum pixel value (default -0.5).\n","        boxmax: Maximum pixel value (default 0.5).\n","        \"\"\"\n","\n","        image_size, num_channels, num_labels = model.image_size, model.num_channels, model.num_labels\n","        self.sess = sess\n","        self.TARGETED = targeted\n","        self.LEARNING_RATE = learning_rate\n","        self.MAX_ITERATIONS = max_iterations\n","        self.BINARY_SEARCH_STEPS = binary_search_steps\n","        self.ABORT_EARLY = abort_early\n","        self.CONFIDENCE = confidence\n","        self.initial_const = initial_const\n","        self.batch_size = batch_size\n","\n","        self.repeat = binary_search_steps >= 10\n","\n","        self.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK = False\n","\n","        shape = (batch_size,image_size,image_size,num_channels)\n","        \n","        # the variable we're going to optimize over\n","        modifier = tf.Variable(np.zeros(shape,dtype=np.float32))\n","\n","        # these are variables to be more efficient in sending data to tf\n","        self.timg = tf.Variable(np.zeros(shape), dtype=tf.float32)\n","        self.tlab = tf.Variable(np.zeros((batch_size,num_labels)), dtype=tf.float32)\n","        self.const = tf.Variable(np.zeros(batch_size), dtype=tf.float32)\n","\n","        # and here's what we use to assign them\n","        self.assign_timg = tf.placeholder(tf.float32, shape)\n","        self.assign_tlab = tf.placeholder(tf.float32, (batch_size,num_labels))\n","        self.assign_const = tf.placeholder(tf.float32, [batch_size])\n","        \n","        # the resulting image, tanh'd to keep bounded from boxmin to boxmax\n","        self.boxmul = (boxmax - boxmin) / 2.\n","        self.boxplus = (boxmin + boxmax) / 2.\n","        self.newimg = tf.tanh(modifier + self.timg) * self.boxmul + self.boxplus\n","        \n","        # prediction BEFORE-SOFTMAX of the model\n","        self.output = model.predict(self.newimg)\n","        \n","        # distance to the input data\n","        self.l2dist = tf.reduce_sum(tf.square(self.newimg-(tf.tanh(self.timg) * self.boxmul + self.boxplus)),[1,2,3])\n","        \n","        # compute the probability of the label class versus the maximum other\n","        real = tf.reduce_sum((self.tlab)*self.output,1)\n","        other = tf.reduce_max((1-self.tlab)*self.output - (self.tlab*10000),1)\n","\n","        if self.TARGETED:\n","            # if targetted, optimize for making the other class most likely\n","            loss1 = tf.maximum(0.0, other-real+self.CONFIDENCE)\n","        else:\n","            # if untargeted, optimize for making this class least likely.\n","            loss1 = tf.maximum(0.0, real-other+self.CONFIDENCE)\n","\n","        # sum up the losses\n","        self.loss2 = tf.reduce_sum(self.l2dist)\n","        self.loss1 = tf.reduce_sum(self.const*loss1)\n","        self.loss = self.loss1+self.loss2\n","        \n","        # Setup the adam optimizer and keep track of variables we're creating\n","        start_vars = set(x.name for x in tf.global_variables())\n","        optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE)\n","        self.train = optimizer.minimize(self.loss, var_list=[modifier])\n","        end_vars = tf.global_variables()\n","        new_vars = [x for x in end_vars if x.name not in start_vars]\n","\n","        # these are the variables to initialize when we run\n","        self.setup = []\n","        self.setup.append(self.timg.assign(self.assign_timg))\n","        self.setup.append(self.tlab.assign(self.assign_tlab))\n","        self.setup.append(self.const.assign(self.assign_const))\n","        \n","        self.init = tf.variables_initializer(var_list=[modifier]+new_vars)\n","\n","    def attack(self, imgs, targets):\n","        \"\"\"\n","        Perform the L_2 attack on the given images for the given targets.\n","\n","        If self.targeted is true, then the targets represents the target labels.\n","        If self.targeted is false, then targets are the original class labels.\n","        \"\"\"\n","        r = []\n","        print('go up to',len(imgs))\n","        for i in range(0,len(imgs),self.batch_size):\n","            print('tick',i)\n","            r.extend(self.attack_batch(imgs[i:i+self.batch_size], targets[i:i+self.batch_size]))\n","        return np.array(r)\n","\n","    def attack_batch(self, imgs, labs):\n","        \"\"\"\n","        Run the attack on a batch of images and labels.\n","        \"\"\"\n","        def compare(x,y):\n","            if not isinstance(x, (float, int, np.int64)):\n","                x = np.copy(x)\n","                if self.TARGETED:\n","                    x[y] -= self.CONFIDENCE\n","                else:\n","                    x[y] += self.CONFIDENCE\n","                x = np.argmax(x)\n","            if self.TARGETED:\n","                return x == y\n","            else:\n","                return x != y\n","\n","        batch_size = self.batch_size\n","\n","        # convert to tanh-space\n","        imgs = np.arctanh((imgs - self.boxplus) / self.boxmul * 0.999999)\n","\n","        # set the lower and upper bounds accordingly\n","        lower_bound = np.zeros(batch_size)\n","        CONST = np.ones(batch_size)*self.initial_const\n","        upper_bound = np.ones(batch_size)*1e10\n","\n","        # the best l2, score, and image attack\n","        o_bestl2 = [1e10]*batch_size\n","        o_bestscore = [-1]*batch_size\n","        o_bestattack = [np.zeros(imgs[0].shape)]*batch_size\n","        \n","        for outer_step in range(self.BINARY_SEARCH_STEPS):\n","            print(o_bestl2)\n","            # completely reset adam's internal state.\n","            self.sess.run(self.init)\n","            batch = imgs[:batch_size]\n","            batchlab = labs[:batch_size]\n","    \n","            bestl2 = [1e10]*batch_size\n","            bestscore = [-1]*batch_size\n","\n","            # The last iteration (if we run many steps) repeat the search once.\n","            if self.repeat == True and outer_step == self.BINARY_SEARCH_STEPS-1:\n","                CONST = upper_bound\n","\n","            # set the variables so that we don't have to send them over again\n","            self.sess.run(self.setup, {self.assign_timg: batch,\n","                                       self.assign_tlab: batchlab,\n","                                       self.assign_const: CONST})\n","            \n","            prev = np.inf\n","            for iteration in range(self.MAX_ITERATIONS):\n","                # perform the attack \n","                _, l, l2s, scores, nimg = self.sess.run([self.train, self.loss, \n","                                                         self.l2dist, self.output, \n","                                                         self.newimg])\n","\n","                if np.all(scores>=-.0001) and np.all(scores <= 1.0001):\n","                    if np.allclose(np.sum(scores,axis=1), 1.0, atol=1e-3):\n","                        if not self.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK:\n","                            raise Exception(\"The output of model.predict should return the pre-softmax layer. It looks like you are returning the probability vector (post-softmax). If you are sure you want to do that, set attack.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK = True\")\n","                \n","                # print out the losses every 10%\n","                if iteration%(self.MAX_ITERATIONS//10) == 0:\n","                    print(iteration,self.sess.run((self.loss,self.loss1,self.loss2)))\n","\n","                # check if we should abort search if we're getting nowhere.\n","                if self.ABORT_EARLY and iteration%(self.MAX_ITERATIONS//10) == 0:\n","                    if l > prev*.9999:\n","                        break\n","                    prev = l\n","\n","                # adjust the best result found so far\n","                for e,(l2,sc,ii) in enumerate(zip(l2s,scores,nimg)):\n","                    if l2 < bestl2[e] and compare(sc, np.argmax(batchlab[e])):\n","                        bestl2[e] = l2\n","                        bestscore[e] = np.argmax(sc)\n","                    if l2 < o_bestl2[e] and compare(sc, np.argmax(batchlab[e])):\n","                        o_bestl2[e] = l2\n","                        o_bestscore[e] = np.argmax(sc)\n","                        o_bestattack[e] = ii\n","\n","            # adjust the constant as needed\n","            for e in range(batch_size):\n","                if compare(bestscore[e], np.argmax(batchlab[e])) and bestscore[e] != -1:\n","                    # success, divide const by two\n","                    upper_bound[e] = min(upper_bound[e],CONST[e])\n","                    if upper_bound[e] < 1e9:\n","                        CONST[e] = (lower_bound[e] + upper_bound[e])/2\n","                else:\n","                    # failure, either multiply by 10 if no solution found yet\n","                    #          or do binary search with the known upper bound\n","                    lower_bound[e] = max(lower_bound[e],CONST[e])\n","                    if upper_bound[e] < 1e9:\n","                        CONST[e] = (lower_bound[e] + upper_bound[e])/2\n","                    else:\n","                        CONST[e] *= 10\n","\n","        # return the best solution found\n","        o_bestl2 = np.array(o_bestl2)\n","        return o_bestattack"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jQZscYPp3cOE"},"source":["# FGSM"]},{"cell_type":"code","metadata":{"id":"vgtLXXyO3emT","executionInfo":{"status":"ok","timestamp":1618088813210,"user_tz":240,"elapsed":191475,"user":{"displayName":"Kevin Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64","userId":"17607607416881687332"}}},"source":["import numpy as np\n","\n","class FGS:\n","    def __init__(self, sess, model, batch_size=100, eps=0.2):\n","        self.sess = sess\n","        self.model = model\n","\n","        self.delta = tf.placeholder(tf.float32, (batch_size,model.image_size,model.image_size,model.num_channels))\n","        self.img = tf.placeholder(tf.float32, (batch_size,model.image_size,model.image_size,model.num_channels))\n","        self.lab = tf.placeholder(tf.float32, (batch_size,model.num_labels))\n","        \n","        self.out = model.predict(tf.clip_by_value(self.img+self.delta, -0.5, 0.5))\n","        self.loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=self.out, \n","                                                                          labels=self.lab))\n","\n","        self.grads = tf.gradients(self.loss,[self.delta])[0]\n","        \n","        self.batch_size = batch_size\n","\n","        self.eps = eps\n","\n","    def attack(self, imgs, labs):\n","        \n","        r = []\n","        \n","        for offset in range(0,len(imgs),self.batch_size):\n","            batch_imgs,batch_labs = imgs[offset:offset+self.batch_size],labs[offset:offset+self.batch_size]\n","        \n","            directions = np.sign(self.sess.run(self.grads, feed_dict={self.img: batch_imgs, \n","                                                                      self.lab: batch_labs,\n","                                                                      self.delta: np.zeros(batch_imgs.shape)}))\n","            \n","            it = np.clip(batch_imgs+directions*self.eps, -.5, .5)\n","\n","            r.extend(it)\n","        return np.array(r)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GYDIsnm1cvUi"},"source":["# Mean filter"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OT85ytnlcw1Y","executionInfo":{"status":"ok","timestamp":1618088855789,"user_tz":240,"elapsed":232008,"user":{"displayName":"Kevin Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64","userId":"17607607416881687332"}},"outputId":"822ddceb-61df-4eb0-c335-b1262c435029"},"source":["## mean_filter.py -- break the mean filter defense\n","##\n","## Copyright (C) 2017, Nicholas Carlini <nicholas@carlini.com>.\n","##\n","## This program is licenced under the BSD 2-Clause licence,\n","## contained in the LICENCE file in this directory.\n","\n","\n","import sys\n","import time\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() \n","import numpy as np\n","import random\n","import scipy.ndimage\n","\n","import sklearn.decomposition\n","\n","from setup_cifar import CIFARModel, CIFAR\n","from setup_mnist import MNISTModel, MNIST\n","\n","from nn_robust_attacks.l2_attack import CarliniL2\n","from fast_gradient_sign import FGS\n","\n","from tensorflow.compat.v1.keras import backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Conv2D\n","from keras.optimizers import SGD\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","\n","def run_filter(Data, Model, path):\n","    K.set_learning_phase(False)\n","    data = Data()\n","    model = Model(path)\n","    model2 = Model(path)\n","\n","    def new_predict(xs):\n","        print(xs.get_shape())\n","        if 'mnist' in path:\n","            xs = tf.nn.conv2d(xs, tf.constant(np.ones((3,3,1,1))/9,dtype=tf.float32),\n","                              [1,1,1,1], \"SAME\")\n","        else:\n","            xs = tf.nn.conv2d(xs, tf.constant(np.ones((3,3,3,3))/9,dtype=tf.float32),\n","                              [1,1,1,1], \"SAME\")\n","        return model2.model(xs)\n","    model2.predict = new_predict\n","\n","    sess = K.get_session()\n","    #dist 1.45976\n","\n","    attack = CarliniL2(sess, model2, batch_size=100, max_iterations=3000,\n","                       binary_search_steps=4, targeted=False, confidence=0,\n","                       initial_const=10)\n","\n","    N = 100\n","\n","    test_adv = attack.attack(data.test_data[:N], data.test_labels[:N])\n","\n","    print('accuracy of original model',np.mean(np.argmax(sess.run(model.predict(tf.constant(data.test_data,dtype=np.float32))),axis=1)==np.argmax(data.test_labels,axis=1)))\n","    print('accuracy of blurred model',np.mean(np.argmax(sess.run(model.predict(tf.constant(data.test_data,dtype=np.float32))),axis=1)==np.argmax(data.test_labels,axis=1)))\n","\n","    print('dist',np.mean(np.sum((test_adv-data.test_data[:N])**2,axis=(1,2,3))**.5))\n","\n","    #it = np.argmax(sess.run(model.predict(tf.constant(test_adv))),axis=1)\n","    #print('success of unblured',np.mean(it==np.argmax(data.test_labels,axis=1)[:N]))\n","    it = np.argmax(sess.run(model2.predict(tf.constant(test_adv))),axis=1)\n","    print('success of blured',np.mean(it==np.argmax(data.test_labels,axis=1)[:N]))\n","    \n","\n","run_filter(MNIST, MNISTModel, \"models/mnist\")\n","# run_filter(CIFAR, CIFARModel, \"models/cifar\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n","  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["(100, 28, 28, 1)\n","go up to 100\n","tick 0\n","[10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0, 10000000000.0]\n","0 (14141.048, 14140.942, 0.105619706)\n","300 (755.05615, 1.0599518, 753.9962)\n","600 (504.02536, 0.6181574, 503.4072)\n","900 (365.95352, 0.026378632, 365.92715)\n","1200 (301.3334, 0.6548548, 300.67856)\n","1500 (269.60828, 0.0, 269.60828)\n","1800 (256.63098, 1.2632513, 255.36774)\n","2100 (247.74991, 0.0, 247.74991)\n","2400 (239.33469, 0.0666523, 239.26804)\n","2700 (239.57314, 0.8911133, 238.68202)\n","[2.108302, 5.7887583, 1.0022511, 4.0956373, 0.68928546, 1.3279989, 1.4699951, 1.1979399, 1.1232996, 3.2651644, 5.087696, 3.1292663, 2.0693636, 4.7316856, 1.275603, 0.2911539, 2.7418015, 4.41948, 0.56912506, 1.5510052, 1.193444, 0.9465586, 3.7003984, 3.0730696, 0.6668557, 2.3064866, 1.0550784, 2.1708295, 4.008539, 0.1895233, 3.911306, 0.25950998, 2.6140826, 1.6655949, 3.4500213, 3.6587276, 1.3530555, 0.8257838, 1.2283946, 1.0364113, 0.1861746, 1.3719223, 1.4415631, 2.2984114, 2.2016644, 0.61349696, 0.74974585, 3.4636188, 1.6395216, 1.120851, 3.1173258, 3.2725258, 1.6313219, 1.2039936, 4.8334255, 3.441749, 3.025116, 0.5640517, 3.6255345, 1.2158203, 1.4102092, 1.4812171, 0.11282011, 0.9318286, 3.7654004, 0.18984324, 1.9040184, 1.2492418, 3.8762598, 5.810967, 1.898133, 7.519375, 1.9284744, 1.5627389, 0.91300684, 2.5828233, 1.5352455, 2.2337499, 0.5207774, 3.9210358, 0.20469072, 3.1140673, 6.624515, 1.7547193, 2.9669807, 3.699228, 2.4048784, 1.1712451, 4.640533, 0.9504954, 2.0514681, 4.6372128, 0.08366982, 1.5988624, 1.2531192, 1.0299172, 0.07408183, 1.435106, 2.6165278, 3.9222202]\n","0 (7070.576, 7070.4707, 0.10561927)\n","300 (618.0629, 1.5294349, 616.5335)\n","600 (368.7927, 0.5868697, 368.2058)\n","900 (276.8711, 1.0355854, 275.8355)\n","1200 (247.9407, 0.28199434, 247.6587)\n","1500 (236.95804, 0.9109044, 236.04713)\n","1800 (231.30298, 0.6656933, 230.63728)\n","2100 (228.60826, 0.2789557, 228.3293)\n","2400 (228.22859, 1.4285028, 226.8001)\n","2700 (225.8075, 0.3476739, 225.45982)\n","[2.0660803, 5.7571297, 0.8063064, 4.064108, 0.6790676, 1.3069777, 1.4537016, 1.1645623, 1.1172806, 3.2557101, 4.653082, 3.0870655, 2.0693636, 4.5846815, 1.2650275, 0.28417736, 2.6926477, 3.27027, 0.55682135, 1.5204363, 1.1785948, 0.9400109, 3.7003984, 2.944203, 0.62435555, 2.2954094, 1.0267181, 2.1592553, 3.8262048, 0.18108897, 3.911306, 0.25394166, 2.5091896, 1.6568675, 3.4174504, 3.6080298, 1.3373251, 0.8132968, 1.1670849, 1.0133692, 0.18051556, 1.3532981, 1.3725487, 2.195617, 2.1978173, 0.5764735, 0.74249923, 3.4318523, 1.6293094, 1.0869004, 2.9738095, 3.2572606, 1.5988414, 1.1902189, 4.4899063, 3.3582215, 2.9623947, 0.5549616, 3.5941947, 1.1997148, 1.3409928, 1.4351227, 0.090103544, 0.9164635, 3.719834, 0.17441005, 1.8538146, 1.2425873, 3.8457658, 5.7856874, 1.8891599, 7.4654346, 1.8745992, 1.5627389, 0.86004275, 2.5509207, 1.4941906, 2.216755, 0.5160019, 3.9082303, 0.111877345, 3.0629392, 6.5246263, 1.6911145, 2.9419787, 3.5480614, 2.3790708, 1.1653053, 4.5864544, 0.92167604, 2.035725, 4.6372128, 0.07919307, 1.5963554, 1.2445862, 1.0226612, 0.072408356, 1.3764553, 2.610259, 3.8887265]\n","0 (3535.3413, 3535.2356, 0.10561837)\n","300 (510.38974, 4.461968, 505.92776)\n","600 (276.76984, 0.732851, 276.037)\n","900 (232.31126, 0.9309876, 231.38028)\n","1200 (223.80373, 0.65083265, 223.1529)\n","1500 (219.91978, 0.362643, 219.55714)\n","1800 (218.25095, 0.609591, 217.64136)\n","2100 (217.62144, 0.87396324, 216.74748)\n","2400 (216.9837, 0.63823104, 216.34547)\n","2700 (216.64142, 0.37233114, 216.26909)\n","[2.0565953, 5.7571297, 0.7220656, 3.4974065, 0.67246705, 1.3048671, 1.4223287, 1.1587012, 1.1172806, 3.2302482, 4.312358, 3.0816474, 2.06183, 4.5708065, 1.2599978, 0.2827153, 2.6868396, 2.7333632, 0.5476774, 1.5134373, 1.1752987, 0.888615, 1.6708238, 2.85751, 0.61409914, 2.2916949, 1.021867, 2.1561131, 3.8153, 0.17905807, 3.889947, 0.2513119, 2.4956899, 1.6554697, 3.4127147, 3.5969775, 1.3353951, 0.808262, 1.1528978, 1.0105016, 0.17837703, 1.350223, 1.3581346, 2.1858234, 2.194999, 0.5696808, 0.7416197, 3.4318523, 1.627666, 1.0806277, 2.9363875, 3.2470262, 1.5880195, 1.1872816, 4.4584193, 3.115034, 2.9517417, 0.55241895, 3.5885413, 1.1871101, 1.3336699, 1.391497, 0.08858572, 0.90946066, 3.7120461, 0.1698661, 1.8033876, 1.2416941, 3.8358831, 5.7791405, 1.8880526, 7.454769, 1.8691764, 1.5627389, 0.8555391, 2.5130858, 1.4842105, 2.2129927, 0.51388216, 3.9082303, 0.108820215, 3.0543103, 6.5246263, 1.6773276, 2.9348352, 3.5458596, 2.3595498, 1.1641154, 4.564023, 0.9163137, 2.030943, 4.4335437, 0.068397306, 1.5942205, 1.2433962, 1.0226612, 0.0717268, 1.3720983, 2.6021905, 3.8856127]\n","0 (1767.7234, 1767.6178, 0.10561656)\n","300 (379.20337, 3.4806485, 375.72272)\n","600 (231.20091, 0.84082276, 230.36009)\n","900 (218.41824, 0.9016609, 217.51659)\n","1200 (215.53003, 0.52254915, 215.00748)\n","1500 (214.88136, 0.64055204, 214.24081)\n","1800 (214.30336, 0.63844115, 213.66492)\n","2100 (213.85379, 0.74484974, 213.10895)\n","2400 (213.98036, 0.91941684, 213.06094)\n","accuracy of original model 0.9922\n","accuracy of blurred model 0.9922\n","dist 1.3369265\n","(100, 28, 28, 1)\n","success of blured 0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DeRtih4qlZVu","executionInfo":{"status":"ok","timestamp":1618088495042,"user_tz":240,"elapsed":1154,"user":{"displayName":"Kevin Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64","userId":"17607607416881687332"}},"outputId":"803b76bf-cbbb-4371-89f4-83b466c292dd"},"source":["%%shell\n","jupyter nbconvert --to html '/content/drive/MyDrive/EECS 598 Adv. ML/Presentation1/presentation.ipynb'"],"execution_count":65,"outputs":[{"output_type":"stream","text":["[NbConvertApp] Converting notebook /content/drive/MyDrive/EECS 598 Adv. ML/Presentation1/presentation.ipynb to html\n","[NbConvertApp] Writing 379913 bytes to /content/drive/MyDrive/EECS 598 Adv. ML/Presentation1/presentation.html\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{"tags":[]},"execution_count":65}]}]}